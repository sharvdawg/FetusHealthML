{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bcb6dc",
   "metadata": {},
   "source": [
    "# Imports and Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import src as tools\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\sharv\\\\184\\\\final\\\\FetusHealthML\\\\Project\\\\fetal_health.csv\")\n",
    "\n",
    "#As discussed in the project proposal, we will experiment with using only the first 7 features that are actual recordings of\n",
    "#the patients monitoring.\n",
    "short_df = df[['baseline value','accelerations','fetal_movement','uterine_contractions'\n",
    "               ,'light_decelerations','severe_decelerations','prolongued_decelerations']]\n",
    "\n",
    "y_labels = df[['fetal_health']]\n",
    "\n",
    "df = df.loc[:,df.columns!=\"fetal_health\"]\n",
    "\n",
    "df = df.to_numpy()\n",
    "short_df = short_df.to_numpy()\n",
    "y_labels = y_labels.to_numpy()\n",
    "y_labels = y_labels-1\n",
    "\n",
    "print(\"Original dataset:\",df.shape[0],\"datapoints,\",df.shape[1],\"features\")\n",
    "print(\"Shortened Dataset:\",short_df.shape[0],\"datapoints,\",short_df.shape[1],\"features\")\n",
    "print(\"True labels of Dataset:\",y_labels.shape)\n",
    "#Thankfully all the values are numerical, no need to reencode them\n",
    "# TODO: MAYBE WANT TO NORMALIZE DATASET? CONSIDER DELETING SEVERE_DECELERATIONS COL SINCE ITS ALMOST ALL 0??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41350b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the dataset prepared, we must split it into training, validation, and testing sets.\n",
    "#Note: The validation set is really only for the Neural Network model. We will do another split for it seperately\n",
    "x_train, x_test, y_train, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train.shape)\n",
    "print(\"Training Targt size\", y_train.shape)\n",
    "print(\"Testing Set size:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60b592",
   "metadata": {},
   "source": [
    "# KNN(small dataset & cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa4e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using the KNNClassifier from the sklearnknn_classifier.fit(X_train, y_train) library, we will experiment with different values of k.\n",
    "\n",
    "KNN_crossValidationErrors = [0]*250\n",
    "\n",
    "x_train = short_df\n",
    "y_train = y_labels\n",
    "\n",
    "\n",
    "display(x_train)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "for k in range(250):\n",
    "    k = k+1 #1-250 not 0-249\n",
    "    if(k%25==0):\n",
    "        print(k/2.5) #Scuffed progress bar\n",
    "    # Cross-validation with 5 fold\n",
    "    nFolds = 5\n",
    "    for iFold in range(nFolds):\n",
    "        Xti, Xvi, Yti, Yvi = tools.crossValidate(x_train, y_train,nFolds, iFold)\n",
    "        knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "        knnClassifier.fit(Xti, Yti)\n",
    "    \n",
    "        cross_validation_pred = knnClassifier.predict(Xvi)\n",
    "    \n",
    "        cross_validation_accuracy = accuracy_score(Yvi,cross_validation_pred)\n",
    "\n",
    "        cross_validation_error = 1-cross_validation_accuracy\n",
    "        KNN_crossValidationErrors[k-1] += cross_validation_error\n",
    "    KNN_crossValidationErrors[k-1] = KNN_crossValidationErrors[k-1]/nFolds\n",
    "\n",
    "plt.semilogx(range(1, len(KNN_crossValidationErrors) + 1), KNN_crossValidationErrors, color='g')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest error rate is\",min(KNN_crossValidationErrors),\"at k =\",KNN_crossValidationErrors.index(min(KNN_crossValidationErrors))+1)\n",
    "# k = KNN_testErrors.index(min(KNN_testErrors))+1 because it's pulling the error rate from a list (starts at 0)\n",
    "#But k values actually start at k = 1\n",
    "bestk = KNN_crossValidationErrors.index(min(KNN_crossValidationErrors))+1\n",
    "\n",
    "\n",
    "#Confusion Matrix for best performing KNN.\n",
    "x_train, x_test, y_train, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train.shape)\n",
    "print(\"Training Targt size\", y_train.shape)\n",
    "print(\"Testing Set size:\",x_test.shape)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train, y_train)\n",
    "y_test_pred = knnClassifier.predict(x_test)\n",
    "matrix = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy score: {}\".format(accuracy_score(y_test,y_test_pred)))\n",
    "print(\"F1 score: {}\".format(f1_score(y_test, y_test_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b650d4e",
   "metadata": {},
   "source": [
    "# KNN(normal dataset & NO cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will repeat the same process, but with the entire dataset. To save time, I will only test for up to k=100. \n",
    "#In all likelihood, the best k value will reveal itself early on\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train2.shape)\n",
    "print(\"Testing Set size:\",x_test2.shape)\n",
    "\n",
    "KNN_testErrors = [] #index will represent k value. Testing up to k = 100\n",
    "KNN_trainErrors = []\n",
    "for k in range(100):\n",
    "    k = k+1 #1-100, not 0-99\n",
    "    if(k%10==0):\n",
    "        print(k) #Scuffed progress bar\n",
    "    knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "    \n",
    "    y_train_pred2 = knnClassifier.predict(x_train2)\n",
    "    y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "    \n",
    "    train_accuracy2 = accuracy_score(y_train2,y_train_pred2)\n",
    "    test_accuracy2 = accuracy_score(y_test2,y_test_pred2)\n",
    "    train_error2 = 1-train_accuracy2\n",
    "    test_error2 = 1-test_accuracy2\n",
    "    KNN_trainErrors.append(train_error2)\n",
    "    KNN_testErrors.append(test_error2)\n",
    "    \n",
    "plt.semilogx(range(1, len(KNN_trainErrors) + 1), KNN_trainErrors, color='g')\n",
    "plt.semilogx(range(1, len(KNN_testErrors) + 1), KNN_testErrors, color='r')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "bestk = KNN_testErrors.index(min(KNN_testErrors))+1\n",
    "print(\"Lowest error rate is\",min(KNN_testErrors),\"at k =\",bestk)\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "\n",
    "matrix = confusion_matrix(y_test2, y_test_pred2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test2,y_test_pred2)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test2, y_test_pred2, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a3874",
   "metadata": {},
   "source": [
    "# KNN(normal dataset & cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will repeat the same process, but with the entire dataset. To save time, I will only test for up to k=100. \n",
    "#In all likelihood, the best k value will reveal itself early on\n",
    "x_train2 = df\n",
    "y_train2 = y_labels\n",
    "print(\"Training Set size:\",x_train2.shape)\n",
    "print(\"Training Labels size:\",)\n",
    "\n",
    "x_train2 = np.array(x_train2)\n",
    "y_train2 = np.array(y_train2)\n",
    "\n",
    "KNN_testErrors = [0]*100 #index will represent k value. Testing up to k = 100\n",
    "KNN_trainErrors = [0]*100\n",
    "for k in range(100):\n",
    "    k = k+1 #1-100, not 0-99\n",
    "    if(k%10==0):\n",
    "        print(k) #Scuffed progress bar\n",
    "    nFolds = 5\n",
    "    for iFold in range(nFolds):\n",
    "        Xti, Xvi, Yti, Yvi = tools.crossValidate(x_train2, y_train2,nFolds, iFold)\n",
    "        knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "        knnClassifier.fit(Xti, Yti)\n",
    "    \n",
    "        y_train_pred2 = knnClassifier.predict(Xvi)\n",
    "    \n",
    "        train_accuracy2 = accuracy_score(Yvi,y_train_pred2)\n",
    "        train_error2 = 1-train_accuracy2\n",
    "        KNN_trainErrors[k-1] += train_error2\n",
    "    KNN_trainErrors[k-1] = KNN_trainErrors[k-1]/5\n",
    "    \n",
    "plt.semilogx(range(1, len(KNN_trainErrors) + 1), KNN_trainErrors, color='g')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "bestk = KNN_trainErrors.index(min(KNN_trainErrors))+1\n",
    "print(\"Lowest cross-validation error rate is\",min(KNN_trainErrors),\"at k =\",bestk)\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "matrix = confusion_matrix(y_test2, y_test_pred2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test2,y_test_pred2)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test2, y_test_pred2, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def9cb0",
   "metadata": {},
   "source": [
    "Overall, KNN had some decent predicting power. As expected, as more features became considered, KNN's prediction accuracy on testing sets declined significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1398e",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll start with a simple one\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel1\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=7))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "f1scoreAvg = []\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(short_df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)\n",
    "    f1scoreAvg.append(f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score Avg:\",sum(f1scoreAvg)/nFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e303f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating same NN model but with the whole dataset\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel2\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=21))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "f1scoreAvg=[]\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    #print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)\n",
    "    f1scoreAvg.append(f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score Avg:\",sum(f1scoreAvg)/nFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef65ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are performing poorly most likely because the data set is heavily imbalanced towards the 0 class compared \n",
    "#to 1 and 2. To address this, the model will be trained with class weights. Entire dataset is still used.\n",
    "\n",
    "#EDIT: THE MODELS KEPT GUESSING ONE LABEL BECAUSE OF SGD OPTIMIZER. ONCE MODIFIED TO ADAM OPTIMIZER, IT STARTED PERFORMING\n",
    "#BETTER. APPROXIMATELY 88 PERCENT ACCURACY.\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "class_weights = dict(zip(unique, counts))\n",
    "#print(class_weights)\n",
    "#class weights created using inverse class frequency method\n",
    "class_weights[0] = y_labels.size/(3*class_weights[0])\n",
    "class_weights[1] =  y_labels.size/(3*class_weights[1])\n",
    "class_weights[2] =  y_labels.size/(3*class_weights[2])\n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the weights, we can recompile the model, and hopefully obtain better results.\n",
    "#Repeating same NN model but with the whole dataset\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel2\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=21))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "f1scoreAvg=[]\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5,class_weight=class_weights)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    #print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)\n",
    "    f1scoreAvg.append(f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score Avg:\",sum(f1scoreAvg)/nFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortuantely, class weights didn't seem to improve the accuracy at all.\n",
    "#Instead of informing the model of class weights, we will be using SMOTE (Synthetic Minority Oversampling Technique) to\n",
    "#forcefully balance the dataset. Hopefully, this will yield more promising results...\n",
    "#After the data is split for a fold, I'll use imblearn's SMOTE algorithm to create a more balanced dataset.\n",
    "#This introduces another variable (how we choose to balance our dataset) \n",
    "#With the weights, we can recompile the model, and hopefully obtain better results.\n",
    "#Repeating same NN model but with the whole dataset\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel2\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=21))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "f1scoreAvg=[]\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(df, y_labels,nFolds, iFold)\n",
    "    #After creating the split for this fold, we must rebalance this dataset\n",
    "    smoteMachine = SMOTE(random_state=42,k_neighbors=7)\n",
    "    XtiResample, YtiResample = smoteMachine.fit_resample(Xti, Yti)\n",
    "    model.fit(XtiResample, YtiResample, epochs=5)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    #print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)\n",
    "    f1scoreAvg.append(f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score Avg:\",sum(f1scoreAvg)/nFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584665a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
