{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bcb6dc",
   "metadata": {},
   "source": [
    "# Imports and Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4f5dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sharv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import src as tools\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e367884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721cc5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 2126 datapoints, 21 features\n",
      "Shortened Dataset: 2126 datapoints, 7 features\n",
      "True labels of Dataset: (2126, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\sharv\\\\184\\\\final\\\\FetusHealthML\\\\Project\\\\fetal_health.csv\")\n",
    "\n",
    "#As discussed in the project proposal, we will experiment with using only the first 7 features that are actual recordings of\n",
    "#the patients monitoring.\n",
    "short_df = df[['baseline value','accelerations','fetal_movement','uterine_contractions'\n",
    "               ,'light_decelerations','severe_decelerations','prolongued_decelerations']]\n",
    "\n",
    "y_labels = df[['fetal_health']]\n",
    "\n",
    "df = df.loc[:,df.columns!=\"fetal_health\"]\n",
    "\n",
    "df = df.to_numpy()\n",
    "short_df = short_df.to_numpy()\n",
    "y_labels = y_labels.to_numpy()\n",
    "y_labels = y_labels-1\n",
    "\n",
    "print(\"Original dataset:\",df.shape[0],\"datapoints,\",df.shape[1],\"features\")\n",
    "print(\"Shortened Dataset:\",short_df.shape[0],\"datapoints,\",short_df.shape[1],\"features\")\n",
    "print(\"True labels of Dataset:\",y_labels.shape)\n",
    "#Thankfully all the values are numerical, no need to reencode them\n",
    "# TODO: MAYBE WANT TO NORMALIZE DATASET? CONSIDER DELETING SEVERE_DECELERATIONS COL SINCE ITS ALMOST ALL 0??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41350b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the dataset prepared, we must split it into training, validation, and testing sets.\n",
    "#Note: The validation set is really only for the Neural Network model. We will do another split for it seperately\n",
    "x_train, x_test, y_train, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train.shape)\n",
    "print(\"Training Targt size\", y_train.shape)\n",
    "print(\"Testing Set size:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array(x_train)\n",
    "#x_test = np.array(x_test)\n",
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60b592",
   "metadata": {},
   "source": [
    "# KNN(small dataset & cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa4e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using the KNNClassifier from the sklearnknn_classifier.fit(X_train, y_train) library, we will experiment with different values of k.\n",
    "\n",
    "KNN_crossValidationErrors = [0]*250\n",
    "\n",
    "x_train = short_df\n",
    "y_train = y_labels\n",
    "\n",
    "\n",
    "display(x_train)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "for k in range(250):\n",
    "    k = k+1 #1-250 not 0-249\n",
    "    if(k%25==0):\n",
    "        print(k/2.5) #Scuffed progress bar\n",
    "    # Cross-validation with 5 fold\n",
    "    nFolds = 5\n",
    "    for iFold in range(nFolds):\n",
    "        Xti, Xvi, Yti, Yvi = tools.crossValidate(x_train, y_train,nFolds, iFold)\n",
    "        knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "        knnClassifier.fit(Xti, Yti)\n",
    "    \n",
    "        cross_validation_pred = knnClassifier.predict(Xvi)\n",
    "    \n",
    "        cross_validation_accuracy = accuracy_score(Yvi,cross_validation_pred)\n",
    "\n",
    "        cross_validation_error = 1-cross_validation_accuracy\n",
    "        KNN_crossValidationErrors[k-1] += cross_validation_error\n",
    "    KNN_crossValidationErrors[k-1] = KNN_crossValidationErrors[k-1]/nFolds\n",
    "\n",
    "plt.semilogx(range(1, len(KNN_crossValidationErrors) + 1), KNN_crossValidationErrors, color='g')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest error rate is\",min(KNN_crossValidationErrors),\"at k =\",KNN_crossValidationErrors.index(min(KNN_crossValidationErrors))+1)\n",
    "# k = KNN_testErrors.index(min(KNN_testErrors))+1 because it's pulling the error rate from a list (starts at 0)\n",
    "#But k values actually start at k = 1\n",
    "bestk = KNN_crossValidationErrors.index(min(KNN_crossValidationErrors))+1\n",
    "\n",
    "\n",
    "#Confusion Matrix for best performing KNN.\n",
    "x_train, x_test, y_train, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train.shape)\n",
    "print(\"Training Targt size\", y_train.shape)\n",
    "print(\"Testing Set size:\",x_test.shape)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train, y_train)\n",
    "y_test_pred = knnClassifier.predict(x_test)\n",
    "matrix = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy score: {}\".format(accuracy_score(y_test,y_test_pred)))\n",
    "print(\"F1 score: {}\".format(f1_score(y_test, y_test_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b650d4e",
   "metadata": {},
   "source": [
    "# KNN(normal dataset & NO cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will repeat the same process, but with the entire dataset. To save time, I will only test for up to k=100. \n",
    "#In all likelihood, the best k value will reveal itself early on\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "print(\"Training Set size:\",x_train2.shape)\n",
    "print(\"Testing Set size:\",x_test2.shape)\n",
    "\n",
    "KNN_testErrors = [] #index will represent k value. Testing up to k = 100\n",
    "KNN_trainErrors = []\n",
    "for k in range(100):\n",
    "    k = k+1 #1-100, not 0-99\n",
    "    if(k%10==0):\n",
    "        print(k) #Scuffed progress bar\n",
    "    knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "    \n",
    "    y_train_pred2 = knnClassifier.predict(x_train2)\n",
    "    y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "    \n",
    "    train_accuracy2 = accuracy_score(y_train2,y_train_pred2)\n",
    "    test_accuracy2 = accuracy_score(y_test2,y_test_pred2)\n",
    "    train_error2 = 1-train_accuracy2\n",
    "    test_error2 = 1-test_accuracy2\n",
    "    KNN_trainErrors.append(train_error2)\n",
    "    KNN_testErrors.append(test_error2)\n",
    "    \n",
    "plt.semilogx(range(1, len(KNN_trainErrors) + 1), KNN_trainErrors, color='g')\n",
    "plt.semilogx(range(1, len(KNN_testErrors) + 1), KNN_testErrors, color='r')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "bestk = KNN_testErrors.index(min(KNN_testErrors))+1\n",
    "print(\"Lowest error rate is\",min(KNN_testErrors),\"at k =\",bestk)\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "\n",
    "matrix = confusion_matrix(y_test2, y_test_pred2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test2,y_test_pred2)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test2, y_test_pred2, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a3874",
   "metadata": {},
   "source": [
    "# KNN(normal dataset & cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will repeat the same process, but with the entire dataset. To save time, I will only test for up to k=100. \n",
    "#In all likelihood, the best k value will reveal itself early on\n",
    "x_train2 = df\n",
    "y_train2 = y_labels\n",
    "print(\"Training Set size:\",x_train2.shape)\n",
    "print(\"Training Labels size:\",)\n",
    "\n",
    "x_train2 = np.array(x_train2)\n",
    "y_train2 = np.array(y_train2)\n",
    "\n",
    "KNN_testErrors = [0]*100 #index will represent k value. Testing up to k = 100\n",
    "KNN_trainErrors = [0]*100\n",
    "for k in range(100):\n",
    "    k = k+1 #1-100, not 0-99\n",
    "    if(k%10==0):\n",
    "        print(k) #Scuffed progress bar\n",
    "    nFolds = 5\n",
    "    for iFold in range(nFolds):\n",
    "        Xti, Xvi, Yti, Yvi = tools.crossValidate(x_train2, y_train2,nFolds, iFold)\n",
    "        knnClassifier = KNeighborsClassifier(n_neighbors=k)\n",
    "        knnClassifier.fit(Xti, Yti)\n",
    "    \n",
    "        y_train_pred2 = knnClassifier.predict(Xvi)\n",
    "    \n",
    "        train_accuracy2 = accuracy_score(Yvi,y_train_pred2)\n",
    "        train_error2 = 1-train_accuracy2\n",
    "        KNN_trainErrors[k-1] += train_error2\n",
    "    KNN_trainErrors[k-1] = KNN_trainErrors[k-1]/5\n",
    "    \n",
    "plt.semilogx(range(1, len(KNN_trainErrors) + 1), KNN_trainErrors, color='g')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n",
    "\n",
    "bestk = KNN_trainErrors.index(min(KNN_trainErrors))+1\n",
    "print(\"Lowest cross-validation error rate is\",min(KNN_trainErrors),\"at k =\",bestk)\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=bestk)\n",
    "knnClassifier.fit(x_train2, y_train2.ravel())\n",
    "y_test_pred2 = knnClassifier.predict(x_test2)\n",
    "matrix = confusion_matrix(y_test2, y_test_pred2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
    "disp.plot()\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test2,y_test_pred2)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test2, y_test_pred2, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def9cb0",
   "metadata": {},
   "source": [
    "Overall, KNN had some decent predicting power. As expected, as more features became considered, KNN's prediction accuracy on testing sets declined significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1398e",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df343f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before creating the models and such, we msut create the training, validation, and testing splits.\n",
    "#x_temp, x_test, y_temp, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.2,stratify=y_temp)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(short_df, y_labels, test_size=0.2,stratify=y_labels)\n",
    "\n",
    "#x_train = np.array(x_train)\n",
    "#x_test = np.array(x_test)\n",
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)\n",
    "#y_train = y_train.astype(int)\n",
    "#y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1886eb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sharv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We'll start with a simple one\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel1\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=7))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7cd588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sharv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"FetusHealthNNmodel1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                312       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 39        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 551 (2.15 KB)\n",
      "Trainable params: 551 (2.15 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f54b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.8289\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.8289\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6147 - accuracy: 0.8289\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.8289\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.5974 - accuracy: 0.8289\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6284 - accuracy: 0.8089\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.8089\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6241 - accuracy: 0.8089\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6228 - accuracy: 0.8089\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.8089\n",
      "14/14 [==============================] - 0s 4ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7768 - accuracy: 0.7282\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7743 - accuracy: 0.7282\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7724 - accuracy: 0.7282\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7710 - accuracy: 0.7282\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7698 - accuracy: 0.7282\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7265 - accuracy: 0.7525\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7262 - accuracy: 0.7525\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7261 - accuracy: 0.7525\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7260 - accuracy: 0.7525\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7258 - accuracy: 0.7525\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.7737\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.7737\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.6745 - accuracy: 0.7737\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6732 - accuracy: 0.7737\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.7737\n",
      "14/14 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(short_df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    #print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score:\",f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e303f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating same NN model but with the whole dataset\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel2\")\n",
    "model.add(layers.Dense(25, activation='relu',input_dim=21))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f164bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FetusHealthNNmodel2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 25)                550       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                312       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 39        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 901 (3.52 KB)\n",
      "Trainable params: 901 (3.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba6b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 1s 4ms/step - loss: 7.0994 - accuracy: 0.8013\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.8289\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.8289\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.8289\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.5908 - accuracy: 0.8289\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.8089\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.6218 - accuracy: 0.8089\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.6209 - accuracy: 0.8089\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.8089\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.8089\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7700 - accuracy: 0.7282\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7663 - accuracy: 0.7282\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7661 - accuracy: 0.7282\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7659 - accuracy: 0.7282\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7662 - accuracy: 0.7282\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7252 - accuracy: 0.7525\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7253 - accuracy: 0.7525\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7253 - accuracy: 0.7525\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7254 - accuracy: 0.7525\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7249 - accuracy: 0.7525\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.7737\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.7737\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.7737\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6644 - accuracy: 0.7737\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.7737\n",
      "14/14 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score:\",f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef65ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 1.2845921450151057, 1.0: 7.206779661016949, 2.0: 12.079545454545455}\n"
     ]
    }
   ],
   "source": [
    "# The models are performing poorly most likely because the data set is heavily imbalanced towards the 0 class compared \n",
    "#to 1 and 2. To address this, the model will be trained with class weights. Entire dataset is still used.\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "class_weights = dict(zip(unique, counts))\n",
    "#print(class_weights)\n",
    "class_weights[0] = y_labels.size/class_weights[0]\n",
    "class_weights[1] =  y_labels.size/class_weights[1]\n",
    "class_weights[2] =  y_labels.size/class_weights[2]\n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c18cd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FetusHealthNNmodel2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 200)               4400      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 35)                3535      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 108       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28143 (109.93 KB)\n",
      "Trainable params: 28143 (109.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#With the weights, we can recompile the model, and hopefully obtain better results.\n",
    "#Repeating same NN model but with the whole dataset\n",
    "model = keras.Sequential(name=\"FetusHealthNNmodel2\")\n",
    "model.add(layers.Dense(200, activation='relu',input_dim=21))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(35, activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa9f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 1s 4ms/step - loss: 19662160841395117339181056.0000 - accuracy: 0.4850\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 2.9169 - accuracy: 0.8289\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 2.9086 - accuracy: 0.8289\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 2.9051 - accuracy: 0.8289\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 2.9040 - accuracy: 0.8289\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.0689 - accuracy: 0.8089\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.0661 - accuracy: 0.8089\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.0636 - accuracy: 0.8089\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.0636 - accuracy: 0.8089\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.0633 - accuracy: 0.8089\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.7550 - accuracy: 0.3088\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.7219 - accuracy: 0.1029\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.7118 - accuracy: 0.1029\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.7080 - accuracy: 0.1029\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.7072 - accuracy: 0.1288\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.5297 - accuracy: 0.1199\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.5286 - accuracy: 0.0982\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.5281 - accuracy: 0.0958\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.5281 - accuracy: 0.0958\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.5276 - accuracy: 0.0958\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.1734 - accuracy: 0.1476\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.1289 - accuracy: 0.1705\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.1120 - accuracy: 0.1705\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.1056 - accuracy: 0.1705\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 3.1032 - accuracy: 0.1705\n",
      "14/14 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "nFolds = 5\n",
    "testingCrossValidationError = []\n",
    "for iFold in range(nFolds):\n",
    "    Xti, Xvi, Yti, Yvi = tools.crossValidate(df, y_labels,nFolds, iFold)\n",
    "    model.fit(Xti, Yti, epochs=5,class_weight=class_weights)\n",
    "    y_test_pred = np.argmax(model.predict(Xvi),axis=1) #to get the prediction based off of the softmax results\n",
    "    #print(y_test_pred)\n",
    "    test_accuracy = accuracy_score(Yvi,y_test_pred)\n",
    "    testingCrossValidationError.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b10cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.25552830709748686\n",
      "F1 Score: 0.00027359781121751026\n"
     ]
    }
   ],
   "source": [
    "avgTestAccuracy = sum(testingCrossValidationError)/nFolds\n",
    "print(\"Testing Accuracy:\",avgTestAccuracy)\n",
    "print(\"F1 Score:\",f1_score(Yvi, y_test_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709419b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
